server:
  port: 8080

spring:
  servlet:
    multipart:
      max-file-size: 50MB
      max-request-size: 50MB

novel:
  llm:
    provider: deepseek # deepseek, ollama, gemini or coze

llm:
  coze:
    base-url: ${COZE_BASE_URL:https://api.coze.com}
    api-key: ${COZE_API_KEY}
    bot-id: ${COZE_BOT_ID}
    user-id: ${COZE_USER_ID:user_default}
    rate-limit:
      enabled: true
      max-requests: 2
      duration-seconds: 60
  deepseek:
    base-url: ${DEEPSEEK_BASE_URL:https://api.deepseek.com}
    api-key: ${DEEPSEEK_API_KEY}
    model: ${DEEPSEEK_MODEL:deepseek-chat}
    rate-limit:
      enabled: true
      max-requests: 3
      duration-seconds: 60
  gemini:
    base-url: ${GEMINI_BASE_URL:https://generativelanguage.googleapis.com}
    api-key: ${GEMINI_API_KEY}
    model: ${GEMINI_MODEL:gemini-2.5-flash}
    rate-limit:
      enabled: true
      max-requests: 15
      duration-seconds: 60
  ollama:
    url: http://localhost:11434
    model: qwen2.5:7b-instruct-q4_K_M
    # 补充适配4060的优化参数（核心）
    options:
      num_ctx: 2048        # 限制上下文窗口，避免显存占用过高
      num_threads: 16      # 适配i5-12600KF的16线程，提升推理速度
      temperature: 0.1     # 降低随机性，缩短响应时间
      max_tokens: 512      # 限制输出长度，减少计算量
      format: json         # 强制JSON输出，匹配你的项目需求
      num_gpu: 1           # 强制使用GPU，避免CPU推理

embedding:
  store:
    type: chroma # memory or chroma
  onnx:
    # 直接指向您现有的文件
    model-path: "d:/soft/novel-splitter/embedding/src/main/resources/embedding/model.onnx"
    # model-path: "D:/path/to/model.onnx"
    # vocab-path: "D:/path/to/vocab.txt"
chroma:
  url: http://localhost:8081
  collection: novel-splitter

assembler:
  max-chunks: 5
  max-chunk-length: 3000
  max-context-tokens: 12000

splitter:
  storage:
    root-path: "d:/soft/novel-splitter/data/novel-storage"
    
  rule:
    target-length: 1200
    min-length: 200
    max-length: 3000
    ignore-empty-lines: true
    
  downloader:
    thread-count: 3
    timeout-ms: 30000
    retry-count: 3
    sites:
      - domain: "www.example.com"
        catalog-url: "https://www.example.com/book/123"
        chapter-list-selector: ".chapter-list a"
        content-selector: "#content"
        next-page-selector: ".next-page"
      - domain: "www.test.com"
        catalog-url: "https://www.test.com/book/456"
        chapter-list-selector: "div.list > a"
        content-selector: "div.read-content"
# "www.xinghuowxw.com":
      #    {
      #        "chapterPath": "/html/body/div[2]/section/div[3]/div/ul",
      #        "checkUrl": "https://www.xinghuowxw.com/book/B7KB.html",
      #        "contentPath": "/html/body/div[2]/main/section/article",
      #        "nextLoop":true,
      #        "nextUrlPath":"/html/body/div[2]/main/div/a[3]",
      #        "nextPathText":"下一页",
      #        "nextLoopCnt":15
      #    }
      - domain: "https://www.xinghuowxw.com"
        catalog-url: "https://www.xinghuowxw.com/book/8T77.html"
        chapter-list-selector: "div.list > a"
        content-selector: "div.read-content"
        next-page-selector: "div.page-nav > a.next"

  rag:
    default-top-k: 5
    min-confidence: 0.5
    max-retries: 2
    output-constraint: "JSON 对象，包含字段：answer, citations (list of {chunkId, reason}), confidence。"
    system-instruction: |
      你是一个专门分析小说内容的智能助手。
      任务：仅根据提供的上下文块回答用户的问题。
      如果上下文中没有答案，请明确说明你不知道。

      *** 重要：输出格式必须是严格的 JSON ***
      不要输出任何 Markdown 标记或代码块。
      不要输出任何多余的解释文字。
      直接返回一个 JSON 对象，Schema 如下：
      {
        "answer": "(string) 这里填写对问题的详细回答",
        "citations": [
          {
            "chunkId": "(string) 引用来源的 chunkId",
            "reason": "(string) 引用理由"
          }
        ],
        "confidence": (number) 0.0 到 1.0
      }

      示例：
      {
        "answer": "楚晨是小说的主角...",
        "citations": [],
        "confidence": 0.9
      }
